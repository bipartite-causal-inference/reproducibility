{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import statsmodels.api as sm\n",
    "import copy \n",
    "import pandas as pd\n",
    "from scipy.sparse import csr_matrix, lil_matrix\n",
    "from tqdm.notebook import tnrange, tqdm\n",
    "from sklearn import metrics\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Helper functions\n",
    "# Check if all elements of array are the same\n",
    "def all_same(a):\n",
    "    return len(set(a)) == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load data\n",
    "data = pd.read_csv('diversion_pp.csv', names=['neigh', 'y', 'zips', 'id', 'dist', 'z'], \n",
    "                   header=0, usecols=[1, 2, 3, 4, 5, 6], low_memory=False)\n",
    "zip_data = pd.read_csv('zip_dta.csv', names=['neigh', 'lon', 'lat', 'zips', 'y', 'closest_int'],\n",
    "                       header=0, usecols=[1, 2, 3, 4, 5, 6])\n",
    "pp_data = pd.read_csv('pp_dta.csv').rename(columns={'Trt': 'z'})\n",
    "pp_data = pp_data.drop(pp_data.columns[0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Power plant data processing\n",
    "num_clusters = len(np.unique(pp_data['neigh']))\n",
    "num_pp = pp_data.groupby('neigh').agg({'neigh': 'count'}).rename(columns={'neigh': 'num_pp'})\n",
    "num_pp.index.names = ['cluster']\n",
    "pp_data = pp_data.join(num_pp, on='cluster').drop(columns=['neigh'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Estimate a GPS model\n",
    "def gps_estimation(pp_data, method='svm'):\n",
    "    clusters = np.unique(pp_data['cluster'])\n",
    "    enc = OneHotEncoder(handle_unknown='ignore')\n",
    "    enc.fit(clusters.reshape(-1, 1))\n",
    "    cluster_one_hot = pd.DataFrame(enc.transform(pp_data.cluster.values.reshape(-1, 1)).toarray())\n",
    "    pp_data_temp = pd.concat([pp_data.drop(columns=['cluster']), cluster_one_hot], axis=1)\n",
    "    \n",
    "    X = pp_data_temp.drop(columns=['z', 'id'])\n",
    "    y = pp_data_temp.z\n",
    "    \n",
    "    if method == 'svm':\n",
    "        param_grid = {'C': [10e0, 5e0, 2e0, 1e0, 1e-1, 1e-2, 1e-3], 'kernel': ['rbf', 'linear']}\n",
    "        mod_svm = GridSearchCV(SVC(probability=True), param_grid=param_grid)\n",
    "        mod_svm.fit(X, y)\n",
    "        gps = mod_svm.predict_proba(X)[:,1].reshape(-1, 1)\n",
    "    elif method == 'forest':\n",
    "        # Number of trees in random forest\n",
    "        n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "        # Number of features to consider at every split\n",
    "        max_features = ['auto', 'sqrt']\n",
    "        # Maximum number of levels in tree\n",
    "        max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "        max_depth.append(None)\n",
    "        # Minimum number of samples required to split a node\n",
    "        min_samples_split = [2, 5, 10]\n",
    "        # Minimum number of samples required at each leaf node\n",
    "        min_samples_leaf = [1, 2, 4]\n",
    "        # Method of selecting samples for training each tree\n",
    "        bootstrap = [True, False]\n",
    "        # Create the random grid\n",
    "        random_grid = {'n_estimators': n_estimators,\n",
    "                       'max_features': max_features,\n",
    "                       'max_depth': max_depth,\n",
    "                       'min_samples_split': min_samples_split,\n",
    "                       'min_samples_leaf': min_samples_leaf,\n",
    "                       'bootstrap': bootstrap}\n",
    "        mod_forest = RandomizedSearchCV(RandomForestClassifier(random_state=666, class_weight='balanced'),\n",
    "                                        param_distributions=random_grid, n_iter=100, cv=5, \n",
    "                                        random_state=666, n_jobs=-1)\n",
    "        mod_forest.fit(X, y)\n",
    "        gps = mod_forest.predict_proba(X)[:,1].reshape(-1, 1)\n",
    "    else:\n",
    "        print('Incorrect prediciton method: {}'.format(method))\n",
    "        return\n",
    "        \n",
    "    return pp_data.assign(gps=gps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_data = gps_estimation(pp_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add a gps column to data\n",
    "def merge_gps_data(data, pp_data):\n",
    "    return data.join(pp_data.gps, on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = merge_gps_data(data, pp_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute exposures given the assignment\n",
    "def get_exposure(data, trt=None):\n",
    "    all_zips = np.unique(data['zips'])\n",
    "    d = {'zips': all_zips, 'e': 0.0, 'p': 0.0}\n",
    "    zips_with_e = pd.DataFrame(d).set_index('zips')\n",
    "    \n",
    "    if trt is not None:\n",
    "        data = data.drop(['z'], axis=1) \\\n",
    "            .join(trt, on='id', how='inner')\n",
    "    \n",
    "    for i in all_zips:\n",
    "        data_i = data.loc[data.zips == i]\n",
    "        #print(np.prod((data_i.gps ** data_i.z) * ((1 - data_i.gps) ** (1 - data_i.z))))\n",
    "        zips_with_e.loc[i].e = np.sum(data_i.dist * data_i.z) / np.sum(data_i.dist)\n",
    "        zips_with_e.loc[i].p = np.prod((data_i.gps ** data_i.z) * ((1 - data_i.gps) ** (1 - data_i.z)))\n",
    "\n",
    "    return zips_with_e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "zips_with_e = get_exposure(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ids = np.unique(data['id'])\n",
    "trt_0 = pd.DataFrame({'all_ids': all_ids, 'z': 0.0})\n",
    "trt_1 = pd.DataFrame({'all_ids': all_ids, 'z': 1.0})\n",
    "zips_with_0 = get_exposure(data, trt_0).rename(columns={'e': 'e_0', 'p': 'p_0'})\n",
    "zips_with_1 = get_exposure(data, trt_1).rename(columns={'e': 'e_1', 'p': 'p_1'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_final = zip_data.join(zips_with_e, on='zips') \\\n",
    "    .join(zips_with_0, on='zips') \\\n",
    "    .join(zips_with_1, on='zips')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Variables\n",
    "def generate_vars(dataset_final):\n",
    "    y = dataset_final.y.to_numpy().reshape((-1,1))\n",
    "    e = dataset_final.e.to_numpy().reshape((-1,1))\n",
    "    gps = dataset_final.p.to_numpy().reshape((-1,1))\n",
    "    gps_at_0 = dataset_final.p_0.to_numpy().reshape((-1,1))\n",
    "    gps_at_1 = dataset_final.p_1.to_numpy().reshape((-1,1))\n",
    "    gps_at = np.column_stack((gps_at_0, gps_at_1))\n",
    "    return (y, e, gps, gps_at)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function that estimates a naive model\n",
    "def est_naive(y, e):\n",
    "    # Matrix of regressors\n",
    "    X = sm.add_constant(e)\n",
    "    # Regress y on exposure and constant\n",
    "    model = sm.OLS(y, X).fit()\n",
    "    params_hat = model.params.reshape((-1,1))\n",
    "    eps_hat = y - model.predict().reshape((-1,1))\n",
    "    return (params_hat, eps_hat)\n",
    "\n",
    "def ate_naive(y, e):\n",
    "    return est_naive(y, e)[0][1]\n",
    "\n",
    "## Function that estimates a kernell ridge regression\n",
    "def ate_kernel(y, e, gps, gps_at):\n",
    "    # Sample size\n",
    "    N = y.size\n",
    "    # Cross-validate the model\n",
    "    param_grid = {\"alpha\": [1e0, 1e-1, 1e-2, 1e-3]}\n",
    "    kr = GridSearchCV(KernelRidge(), param_grid=param_grid)\n",
    "    X = np.column_stack((e, gps))\n",
    "    kr.fit(X, y)\n",
    "    # Get the predictions\n",
    "    X_0 = np.column_stack((np.zeros((N,1)), gps_at[:,0]))\n",
    "    y_0 = kr.predict(X_0)\n",
    "    X_1 = np.column_stack((np.ones((N,1)), gps_at[:,-1]))\n",
    "    y_1 = kr.predict(X_1)\n",
    "    return np.mean(y_1 - y_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Bootstrap CIs\n",
    "def compute_bootstrap_CI(y, boot_iter_func, est_func, *args, num_boot_iter=1000, alpha=0.05):\n",
    "    # Check if all elements are the same\n",
    "    # Bootstrap iterations\n",
    "    beta_hat_boot_dist_unsorted = np.zeros(num_boot_iter)\n",
    "    for b in tnrange(num_boot_iter, desc='Bootstrap'): #tqdm_notebook(range(num_boot_iter), desc='Bootsrap:'):\n",
    "        beta_hat_boot_dist_unsorted[b] = boot_iter_func(y, est_func, *args)\n",
    "    beta_hat_boot_dist = np.sort(beta_hat_boot_dist_unsorted)\n",
    "    # Sample observations\n",
    "    q_lo = beta_hat_boot_dist[int(np.floor(num_boot_iter * alpha / 2.0))]\n",
    "    q_hi = beta_hat_boot_dist[min(int(np.ceil(num_boot_iter * (1.0 - alpha / 2.0))), \n",
    "                                  num_boot_iter - 1)] # check for out-of-bounds\n",
    "    return (q_lo, q_hi)\n",
    "\n",
    "## Naive bootstrap iteration\n",
    "def bootstrap_naive_iter(y, est_func, e, *args):\n",
    "    # Sample size\n",
    "    N = y.size\n",
    "    while True:\n",
    "        # Sample\n",
    "        boot_sample = np.random.choice(N, N, replace=True)\n",
    "        # Bootstrap data\n",
    "        y_b = y[boot_sample,0] # Bootstrap outcomes\n",
    "        e_b = e[boot_sample,0] # Bootstrap exposures\n",
    "        if not all_same(e_b):\n",
    "            break\n",
    "    args_b = (a if callable(a) else a[boot_sample,:] for a in args)\n",
    "    return est_func(y_b, e_b, *args_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(data, num_boot_iter=1000, alpha=0.05, seed=666):\n",
    "    # Set seed\n",
    "    np.random.seed(seed)\n",
    "    # Get data\n",
    "    (y, e, gps, gps_at) = generate_vars(data)\n",
    "    # Naive estimate\n",
    "    ate_naive_val = ate_naive(y, e)\n",
    "    # Kernel Ridge estimate\n",
    "    ate_kernel_val = ate_kernel(y, e, gps, gps_at)\n",
    "    # Naive bootstrap naive estimate\n",
    "    (q_lo, q_hi) = compute_bootstrap_CI(y, bootstrap_naive_iter, ate_naive, e, num_boot_iter=num_boot_iter, alpha=alpha)\n",
    "    ci_naive_naive = [q_lo, q_hi]\n",
    "    # Naive bootstrap kernel estimate\n",
    "    (q_lo, q_hi) = compute_bootstrap_CI(y, bootstrap_naive_iter, ate_kernel, e, gps, gps_at, num_boot_iter=num_boot_iter, alpha=alpha)\n",
    "    ci_naive_kernel = [q_lo, q_hi]\n",
    "    # Print the results\n",
    "    print('Naive estimate: {}. Kernel ridge estimate: {}.' \\\n",
    "         .format(ate_naive_val, ate_kernel_val))\n",
    "    print('Naive CI: [{},{}]. Kernel ridge CI: [{},{}].' \\\n",
    "         .format(ci_naive_naive[0], ci_naive_naive[1], ci_naive_kernel[0], ci_naive_kernel[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbc4a1c8a7a44f7f89506313683b7708",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Bootstrap', max=200.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0259036870c4ae2baaa40951ea321c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Bootstrap', max=200.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Run the main function\n",
    "main(dataset_final, num_boot_iter=1000, alpha=0.05, seed=666)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
