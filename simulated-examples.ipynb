{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import statsmodels.api as sm\n",
    "import copy \n",
    "import pandas as pd\n",
    "from scipy.sparse import csr_matrix, lil_matrix\n",
    "from tqdm.notebook import tnrange, tqdm\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Helper functions\n",
    "# Check if all elements of array are the same\n",
    "def all_same(a):\n",
    "    return len(set(a)) == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Functions that generates data\n",
    "# Graph\n",
    "def generate_graph_ama(path, data_limiter, N_limiter):\n",
    "    # Load graph data\n",
    "    data = pd.read_csv(path, names=['user', 'item', 'rating', 'time'],\n",
    "                       usecols=[0, 1])\n",
    "    ## Data cleaning\n",
    "    data = data[:data_limiter]\n",
    "    # Get number of reviews\n",
    "    user_reviews = data.groupby('user').size().reset_index(name='user_reviews') \\\n",
    "        .sort_values(by='user_reviews', ascending=False).head(N_limiter)\n",
    "    # Join graph and reviews\n",
    "    data = data.join(user_reviews.set_index('user'), on='user') \\\n",
    "        .sort_values(by=['user_reviews', 'user', 'item']).dropna()\n",
    "    # Number of users and items\n",
    "    N = data.user.unique().size\n",
    "    K = data.item.unique().size\n",
    "    # Define id's\n",
    "    data['user_id'] = data.groupby(['user']).ngroup()\n",
    "    data['item_id'] = data.groupby(['item']).ngroup()\n",
    "    # Lil matrices of adjacency and weights\n",
    "    A_lil = lil_matrix((N, K), dtype=np.float32)\n",
    "    W_lil = lil_matrix((N, K), dtype=np.float32)\n",
    "    for _, row in data.iterrows():\n",
    "        A_lil[row['user_id'], row['item_id']] = 1.0\n",
    "        W_lil[row['user_id'], row['item_id']] = 1.0 / row['user_reviews']\n",
    "    # Sparse matrix of weights\n",
    "    A = A_lil.tocsr()\n",
    "    W = W_lil.tocsr()\n",
    "    # Number of edges\n",
    "    W_num = np.array(copy.deepcopy(data.drop_duplicates(['user_id']).sort_values(by='user_id')['user_reviews'])).reshape((N,1))\n",
    "    return (A, W, W_num)\n",
    "\n",
    "# Graph\n",
    "def generate_graph_sim(N, K_infl, K_noninfl, pi_infl, edge_min, edge_max):\n",
    "    # Total number of diversion units\n",
    "    K = K_infl + K_noninfl\n",
    "    # Adjacency matrix\n",
    "    A = np.zeros((N,K))\n",
    "    # Graph weights\n",
    "    W_num = np.zeros((N,1))\n",
    "    W = np.zeros((N,K))\n",
    "    # Connections to influential units\n",
    "    A[:,:K_infl] = np.random.binomial(1, pi_infl, size=(N,K_infl))\n",
    "    # For each outcome unit 1,...,N:\n",
    "    for i in range(N):\n",
    "        # Number of influential diversion units outcome unit i is connected to\n",
    "        W_infl_num_i = np.sum(A[i,:K_infl])\n",
    "        # Number of non-influential diversion units outcome unit i is connected to\n",
    "        W_noninfl_num_i = max(min(np.random.randint(edge_max + 1), K_noninfl), int(edge_min - W_infl_num_i))\n",
    "        # Number of all diversion units outcome unit i is connected to\n",
    "        W_num_i = W_infl_num_i + W_noninfl_num_i\n",
    "        W_num[i] = W_num_i\n",
    "        # Which ones out of K_infl+1,...,K\n",
    "        if W_noninfl_num_i > 0:\n",
    "            auctions_i = K_infl + np.random.choice(K_noninfl, W_noninfl_num_i, replace=False)\n",
    "            # Adjacency matrix\n",
    "            A[i,auctions_i] = 1.0\n",
    "        # Graph weights (all equal)\n",
    "        W[i,:] = A[i,:] / W_num_i\n",
    "    return (A, W, W_num)\n",
    "\n",
    "# Variables\n",
    "def generate_vars(mu, A, W, W_num, p, sig_xi, sig_epsilon, e_steps):\n",
    "    # Range of exposure values to consider\n",
    "    e_range = np.linspace(0, 1, e_steps)\n",
    "    # Sample sizes\n",
    "    (N, K) = A.shape\n",
    "    # Treatment effects for all outcome units\n",
    "    beta = np.empty((N,1))\n",
    "    # For each outcome unit 1,...,N:\n",
    "    for i in range(N):\n",
    "        beta[i] = mu(1, W_num[i]) - mu(0, W_num[i])\n",
    "    # Normalize beta's\n",
    "    #beta /= np.mean(beta)\n",
    "    while True:\n",
    "        # Assignment to treatment for divertion units\n",
    "        Z = np.random.binomial(1, p, size=(K,1))\n",
    "        # Exposures\n",
    "        e = W.dot(Z).reshape((N,1))\n",
    "        if not all_same(e.flatten()):\n",
    "            break\n",
    "    # Treated edges\n",
    "    W_nonzero = np.rint(e * W_num)\n",
    "    # Generalized propensity score (observed)\n",
    "    gps = stats.binom.pmf(W_nonzero, W_num, p)\n",
    "    # Generalized propensity score (at different exposures)\n",
    "    gps_at = np.zeros((N,e_steps))\n",
    "    for k in range(e_steps):\n",
    "        gps_at[:,k] = stats.binom.pmf(np.rint(e_range[k] * W_num), W_num, p).reshape(N)\n",
    "    # Error terms at the level of diversion units\n",
    "    xi = W.dot(np.random.normal(0, sig_xi, (K,1))).reshape((N,1))\n",
    "    # Outcomes (constant + treatment_effect * exposure + noise)\n",
    "    y = 0 + mu(e, W_num) + xi + np.random.normal(0, sig_epsilon, (N,1))\n",
    "    return (y, beta, Z, e, gps, gps_at)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function that estimates the true ATE\n",
    "def ate_true(beta):\n",
    "    return np.mean(beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function that estimates a naive model\n",
    "def est_naive(y, e):\n",
    "    # Matrix of regressors\n",
    "    X = sm.add_constant(e)\n",
    "    # Regress y on exposure and constant\n",
    "    model = sm.OLS(y, X).fit()\n",
    "    params_hat = model.params.reshape((-1,1))\n",
    "    eps_hat = y - model.predict().reshape((-1,1))\n",
    "    return (params_hat, eps_hat)\n",
    "\n",
    "def ate_naive(y, e):\n",
    "    return est_naive(y, e)[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function that estimates the correctly specified model\n",
    "def ate_correct(y, e, W_num, mu):\n",
    "    # Matrix of regressors\n",
    "    X = sm.add_constant(mu(e, W_num))\n",
    "    # Regress y on exposure and constant\n",
    "    model = sm.OLS(y, X).fit()\n",
    "    # Estimated parameter\n",
    "    b = model.params[1]\n",
    "    N = W_num.size\n",
    "    e_0 = np.zeros((N,1))\n",
    "    e_1 = np.ones((N,1))\n",
    "    return b * np.mean(mu(e_1, W_num) - mu(e_0, W_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function that estimates a kernell ridge regression\n",
    "def ate_kernel(y, e, gps, gps_at):\n",
    "    # Sample size\n",
    "    N = y.size\n",
    "    # Cross-validate the model\n",
    "    param_grid = {\"alpha\": [1e0, 1e-1, 1e-2, 1e-3]}\n",
    "    kr = GridSearchCV(KernelRidge(), param_grid=param_grid)\n",
    "    X = np.column_stack((e, gps))\n",
    "    kr.fit(X, y)\n",
    "    # Get the predictions\n",
    "    X_0 = np.column_stack((np.zeros((N,1)), gps_at[:,0]))\n",
    "    y_0 = kr.predict(X_0)\n",
    "    X_1 = np.column_stack((np.ones((N,1)), gps_at[:,-1]))\n",
    "    y_1 = kr.predict(X_1)\n",
    "    #print('{} {}'.format(np.mean(y_1), np.mean(y_0)))\n",
    "    return np.mean(y_1 - y_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function that estimates a model with \"random coefficients\"\n",
    "def est_random(y, W, Z):\n",
    "    # Matrix of regressors\n",
    "    X = sm.add_constant(W)\n",
    "    # Regress y on weights and a constant\n",
    "    model = sm.OLS(y, X).fit()\n",
    "    alpha_hat = model.params[0]\n",
    "    xi_hat = model.params[1:].reshape((-1,1))\n",
    "    eps_hat = y - model.predict().reshape((-1,1))\n",
    "    # Estimate the small model for the right side\n",
    "    X_small = sm.add_constant(Z)\n",
    "    model_small = sm.OLS(xi_hat, X_small).fit()\n",
    "    params_small_hat = model_small.params.reshape((-1,1))\n",
    "    gamma_hat = xi_hat - model_small.predict().reshape((-1,1))\n",
    "    return (params_small_hat, alpha_hat, gamma_hat, eps_hat)\n",
    "\n",
    "def ate_random(y, W, Z):\n",
    "    return est_random(y, W, Z)[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Bootstrap CIs\n",
    "def compute_bootstrap_CI(y, boot_iter_func, est_func, *args, num_boot_iter=1000, alpha=0.05):\n",
    "    # Check if all elements are the same\n",
    "    # Bootstrap iterations\n",
    "    beta_hat_boot_dist_unsorted = np.zeros(num_boot_iter)\n",
    "    for b in range(num_boot_iter): #tqdm_notebook(range(num_boot_iter), desc='Bootsrap:'):\n",
    "        beta_hat_boot_dist_unsorted[b] = boot_iter_func(y, est_func, *args)\n",
    "    beta_hat_boot_dist = np.sort(beta_hat_boot_dist_unsorted)\n",
    "    # Sample observations\n",
    "    q_lo = beta_hat_boot_dist[int(np.floor(num_boot_iter * alpha / 2.0))]\n",
    "    q_hi = beta_hat_boot_dist[min(int(np.ceil(num_boot_iter * (1.0 - alpha / 2.0))), \n",
    "                                  num_boot_iter - 1)] # check for out-of-bounds\n",
    "    return (q_lo, q_hi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Naive bootstrap iteration\n",
    "def bootstrap_naive_iter(y, est_func, e, *args):\n",
    "    # Sample size\n",
    "    N = y.size\n",
    "    while True:\n",
    "        # Sample\n",
    "        boot_sample = np.random.choice(N, N, replace=True)\n",
    "        # Bootstrap data\n",
    "        y_b = y[boot_sample,0] # Bootstrap outcomes\n",
    "        e_b = e[boot_sample,0] # Bootstrap exposures\n",
    "        if not all_same(e_b):\n",
    "            break\n",
    "    args_b = (a if callable(a) else a[boot_sample,:] for a in args)\n",
    "    return est_func(y_b, e_b, *args_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parametric bootstrap iteration\n",
    "def bootstrap_parametric_iter(y, est_func, W, p, params_small_hat, alpha_hat, gamma_hat, eps_hat):\n",
    "    # Sample sizes\n",
    "    (N, K) = W.shape\n",
    "    while True:\n",
    "        # Bootstrap samples\n",
    "        boot_sample_out = np.random.choice(N, N, replace=True)\n",
    "        boot_sample_div = np.random.choice(K, K, replace=True)\n",
    "        # Bootstrap data\n",
    "        Z_b = np.random.binomial(1, p, size=(K,1)) # Sample assignments\n",
    "        gamma_hat_b = gamma_hat[boot_sample_div,:] # Random effects\n",
    "        xi_hat_b = sm.add_constant(Z_b).dot(params_small_hat) + gamma_hat_b # Main regressor\n",
    "        eps_hat_b = eps_hat[boot_sample_out,:] # Bootstrap error terms\n",
    "        if not all_same(xi_hat_b.reshape(K)):\n",
    "            y_b = alpha_hat + W.dot(xi_hat_b) + eps_hat_b # Recompute outcomes\n",
    "            break\n",
    "    return est_func(y_b, W, Z_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_boot(generate_graph_func, mu,\n",
    "              e_steps=2, p=0.5, sig_xi=0.25, sig_epsilon=0.25, \n",
    "              num_sim_iter=1000, num_boot_iter=1000, alpha=0.05, seed=666,\n",
    "              *args):\n",
    "    # Set seed\n",
    "    np.random.seed(seed)\n",
    "    # Initialize CI variables\n",
    "    (ci_naive, ci_param) = (np.zeros((num_sim_iter,2)), np.zeros((num_sim_iter,2)))\n",
    "    # True value of the parameter\n",
    "    ate_true_val = np.zeros((num_sim_iter,1))\n",
    "    # Generate graph\n",
    "    (A, W, W_num) = generate_graph_func(*args)\n",
    "    # Main loop for simulations\n",
    "    for i in tnrange(num_sim_iter, desc='Simulations'):\n",
    "        (y, beta, Z, e, gps, gps_at) = generate_vars(mu, A, W, W_num, p, sig_xi, sig_epsilon, e_steps)\n",
    "        # True value\n",
    "        ate_true_val[i,0] = ate_true(beta)\n",
    "        # Estimates\n",
    "        (params_small_hat, alpha_hat, gamma_hat, eps_hat) = est_random(y, W, Z)\n",
    "        # Naive bootstrap\n",
    "        (q_lo, q_hi) = compute_bootstrap_CI(y, bootstrap_naive_iter, ate_naive, e, num_boot_iter=num_boot_iter, alpha=alpha)\n",
    "        ci_naive[i,:] = [q_lo, q_hi]\n",
    "        # Parametric bootstrap\n",
    "        (q_lo, q_hi) = compute_bootstrap_CI(y, bootstrap_parametric_iter, ate_random, W, p, params_small_hat, alpha_hat, gamma_hat, eps_hat, num_boot_iter=num_boot_iter, alpha=alpha)\n",
    "        ci_param[i] = [q_lo, q_hi]\n",
    "    \n",
    "    # Coverage / width\n",
    "    (cov_naive, cov_param) = (np.zeros((num_sim_iter,1)), np.zeros((num_sim_iter,1)))\n",
    "    for i in range(num_sim_iter):\n",
    "        cov_naive[i,0] = ci_naive[i,0] <= ate_true_val[i] <= ci_naive[i,1]\n",
    "        cov_param[i,0] = ci_param[i,0] <= ate_true_val[i] <= ci_param[i,1]\n",
    "    width_naive = ci_naive[:,1] - ci_naive[:,0]\n",
    "    width_param = ci_param[:,1] - ci_param[:,0]\n",
    "        \n",
    "    # Print the results\n",
    "    print('Naive bootstrap coverage: {}. Parametric bootstrap coverage: {}' \\\n",
    "         .format(np.mean(cov_naive), np.mean(cov_param)))\n",
    "    print('Naive mean CI width: {}. Parametric mean CI width: {}' \\\n",
    "         .format(np.mean(width_naive), np.mean(width_param)))\n",
    "    print('Naive variance of CI width: {}. Parametric variance of CI width: {}' \\\n",
    "         .format(np.var(width_naive), np.var(width_param)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_bias(generate_graph_func, mu,\n",
    "              e_steps=2, p=0.5, sig_xi=0.0, sig_epsilon=0.25, \n",
    "              num_sim_iter=1000, num_boot_iter=1000, alpha=0.05, seed=666,\n",
    "              *args):\n",
    "    # Set seed\n",
    "    np.random.seed(seed)\n",
    "    # Initialize CI variables\n",
    "    (ci_naive_naive, ci_naive_correct, ci_naive_kernel) = (np.zeros((num_sim_iter,2)), np.zeros((num_sim_iter,2)), np.zeros((num_sim_iter,2)))\n",
    "    # True value of the parameter\n",
    "    ate_true_val = np.zeros((num_sim_iter,1))\n",
    "    # Estimates of the parameter\n",
    "    (ate_naive_val, ate_correct_val, ate_kernel_val) = (np.zeros((num_sim_iter,1)), np.zeros((num_sim_iter,1)), np.zeros((num_sim_iter,1)))\n",
    "    # Generate graph\n",
    "    (A, W, W_num) = generate_graph_func(*args)\n",
    "    # Main loop for simulations\n",
    "    for i in tnrange(num_sim_iter, desc='Simulations'):\n",
    "        (y, beta, Z, e, gps, gps_at) = generate_vars(mu, A, W, W_num, p, sig_xi, sig_epsilon, e_steps)\n",
    "        # True value\n",
    "        ate_true_val[i,0] = ate_true(beta)\n",
    "        # Naive estimate\n",
    "        ate_naive_val[i,0] = ate_naive(y, e)\n",
    "        # Correct estimate\n",
    "        ate_correct_val[i,0] = ate_correct(y, e, W_num, mu)\n",
    "        # Kernel Ridge estimate\n",
    "        ate_kernel_val[i,0] = ate_kernel(y, e, gps, gps_at)\n",
    "        # Estimates\n",
    "        # Naive bootstrap naive estimate\n",
    "        (q_lo, q_hi) = compute_bootstrap_CI(y, bootstrap_naive_iter, ate_naive, e, num_boot_iter=num_boot_iter, alpha=alpha)\n",
    "        ci_naive_naive[i,:] = [q_lo, q_hi]\n",
    "        # Naive bootstrap correct estimate\n",
    "        (q_lo, q_hi) = compute_bootstrap_CI(y, bootstrap_naive_iter, ate_correct, e, W_num, mu, num_boot_iter=num_boot_iter, alpha=alpha)\n",
    "        ci_naive_correct[i,:] = [q_lo, q_hi]\n",
    "        # Naive bootstrap kernel estimate\n",
    "        (q_lo, q_hi) = compute_bootstrap_CI(y, bootstrap_naive_iter, ate_kernel, e, gps, gps_at, num_boot_iter=num_boot_iter, alpha=alpha)\n",
    "        ci_naive_kernel[i,:] = [q_lo, q_hi]\n",
    "    \n",
    "    # Coverage / width\n",
    "    (cov_naive_naive, cov_naive_correct, cov_naive_kernel) = (np.zeros((num_sim_iter,1)), np.zeros((num_sim_iter,1)), np.zeros((num_sim_iter,1)))\n",
    "    for i in range(num_sim_iter):\n",
    "        cov_naive_naive[i,0] = ci_naive_naive[i,0] <= ate_true_val[i] <= ci_naive_naive[i,1]\n",
    "        cov_naive_correct[i,0] = ci_naive_correct[i,0] <= ate_true_val[i] <= ci_naive_correct[i,1]\n",
    "        cov_naive_kernel[i,0] = ci_naive_kernel[i,0] <= ate_true_val[i] <= ci_naive_kernel[i,1]\n",
    "    \n",
    "    print('Naive bias: {}. Correct specification bias: {}. Kernel bias: {}.' \\\n",
    "         .format(np.mean(ate_naive_val - ate_true_val), np.mean(ate_correct_val - ate_true_val), np.mean(ate_kernel_val - ate_true_val)))\n",
    "    print('Std. naive bias: {}. Std. correct specification bias: {}. Std. kernel bias: {}.' \\\n",
    "         .format(np.std(ate_naive_val - ate_true_val), np.std(ate_correct_val - ate_true_val), np.std(ate_kernel_val - ate_true_val)))\n",
    "    print('Naive MRSE: {}. Correct specification MRSE: {}. Kernel MRSE: {}.' \\\n",
    "         .format(np.sqrt(np.mean((ate_naive_val - ate_true_val) ** 2)), np.sqrt(np.mean((ate_correct_val - ate_true_val) ** 2)), np.sqrt(np.mean((ate_kernel_val - ate_true_val) ** 2))))\n",
    "    print('Naive bootstrap naive estimate coverage: {}. Naive bootstrap correct specification estimate coverage: {}. Naive bootstrap kernel estimate coverage: {}.' \\\n",
    "         .format(np.mean(cov_naive_naive), np.mean(cov_naive_correct), np.mean(cov_naive_kernel)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parameters\n",
    "path, data_limiter, N_limiter = 'item_dedup_truncated.csv', 50_000, 1_000\n",
    "N, K_infl, K_noninfl, pi_infl, edge_min, edge_max = 1_000, 0, 100, 0.0, 1, 10\n",
    "sig_xi_uncorrelated = 0.0\n",
    "sig_xi_correlated = 0.25\n",
    "sig_epsilon = 0.25\n",
    "e_steps = 2\n",
    "p = 0.5\n",
    "num_sim_iter = 100\n",
    "num_boot_iter = 200\n",
    "alpha = 0.05\n",
    "seed = 666\n",
    "def mu_homogeneous(e, W_num):\n",
    "    return e * ((edge_min + edge_max) / 2.0)\n",
    "def mu_heterogeneous(e, W_num):\n",
    "    return e * W_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Simulated data, homogeneous treatments\n",
    "main_bias(# Common parameters\n",
    "          generate_graph_sim, mu_homogeneous,\n",
    "          e_steps, p, sig_xi_uncorrelated, sig_epsilon, \n",
    "          num_sim_iter, num_boot_iter, alpha, seed,\n",
    "          # Data parameters\n",
    "          N, K_infl, K_noninfl, pi_infl, edge_min, edge_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Simulated data, heterogeneous treatments\n",
    "main_bias(# Common parameters\n",
    "          generate_graph_sim, mu_heterogeneous,\n",
    "          e_steps, p, sig_xi_uncorrelated, sig_epsilon, \n",
    "          num_sim_iter, num_boot_iter, alpha, seed,\n",
    "          # Data parameters\n",
    "          N, K_infl, K_noninfl, pi_infl, edge_min, edge_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Amazon graph, homogeneous treatments\n",
    "main_bias(# Common parameters\n",
    "          generate_graph_ama, mu_homogeneous,\n",
    "          e_steps, p, sig_xi_uncorrelated, sig_epsilon, \n",
    "          num_sim_iter, num_boot_iter, alpha, seed,\n",
    "          # Data parameters\n",
    "          path, data_limiter, N_limiter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Amazon graph, heterogeneous treatments\n",
    "main_bias(# Common parameters\n",
    "          generate_graph_ama, mu_heterogeneous,\n",
    "          e_steps, p, sig_xi_uncorrelated, sig_epsilon, \n",
    "          num_sim_iter, num_boot_iter, alpha, seed,\n",
    "          # Data parameters\n",
    "          path, data_limiter, N_limiter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Simulated graph, parametric bootstrap for correlated errors\n",
    "main_boot(# Common parameters\n",
    "          generate_graph_sim, mu_homogeneous,\n",
    "          e_steps, p, sig_xi_correlated, sig_epsilon,\n",
    "          num_sim_iter, num_boot_iter, alpha, seed,\n",
    "          # Data parameters\n",
    "          N, K_infl, K_noninfl, pi_infl, edge_min, edge_max)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
